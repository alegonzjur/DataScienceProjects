{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf90e204",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction P2\n",
    "\n",
    "This is a new notebook to work with previous files from the notebook \"model1_p1.ipynb\". I will continue dividing the data between train & test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57bdeb",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "\n",
    "Starting by loading the dataset and splitting it into X and y (Features and target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3ef3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "import numpy as np          \n",
    "import pickle\n",
    "from datetime import datetime      \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a659f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining global variables that will be used to save the state.\n",
    "df, X, y = None, None, None \n",
    "X_train, X_test, y_train, y_test, X_train_balanced, y_train_balanced = None, None, None, None, None, None \n",
    "scaler, encoding_info, feature_importance = None, None, None\n",
    "models = {}\n",
    "results = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f7ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_file, objects=None):\n",
    "    \"\"\"Load the preprocessed dataset and its objects.\"\"\"\n",
    "    \n",
    "    global df, X, y, scaler, encoding_info, feature_importance\n",
    "    \n",
    "    try:\n",
    "        # Loading dataset as dataframe.\n",
    "        df = pd.read_csv(csv_file)\n",
    "        print(f'Dataset loaded successfully!')\n",
    "        print(f'\\nRows: {df.shape[0]}')\n",
    "        print(f'\\nColumns: {df.shape[1]}')\n",
    "        \n",
    "        # Loading objects.\n",
    "        if objects:\n",
    "            with open(objects, 'rb') as f:\n",
    "                objects = pickle.load(f)\n",
    "                scaler = objects.get('scaler')\n",
    "                encoding_info = objects.get('encoding_Ã¯nfo')\n",
    "                feature_importance = objects.get('feature_importance')\n",
    "            print(f'Objects loaded from: {objects}')\n",
    "        \n",
    "        # Splitting in X and y.\n",
    "        if 'Churn' in df.columns:\n",
    "            X = df.drop(columns=['Churn'], axis=1)\n",
    "            y = df['Churn']\n",
    "            print(f'Features (X): {X.shape[0]}')\n",
    "            print(f'Target (y): {y.shape}')\n",
    "        else:\n",
    "            print(f'Churn column not found.')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Encountered error while loading data: {str(e)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800afae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "\n",
      "Rows: 7021\n",
      "\n",
      "Columns: 31\n",
      "Objects loaded from: {'scaler': StandardScaler(), 'encoding_info': {'MultipleLines': {'type': 'onehot', 'columns': ['MultipleLines_No', 'MultipleLines_No phone service', 'MultipleLines_Yes']}, 'InternetService': {'type': 'onehot', 'columns': ['InternetService_DSL', 'InternetService_Fiber optic', 'InternetService_No']}, 'OnlineSecurity': {'type': 'onehot', 'columns': ['OnlineSecurity_No', 'OnlineSecurity_No internet service', 'OnlineSecurity_Yes']}, 'OnlineBackup': {'type': 'onehot', 'columns': ['OnlineBackup_No', 'OnlineBackup_No internet service', 'OnlineBackup_Yes']}, 'DeviceProtection': {'type': 'onehot', 'columns': ['DeviceProtection_No', 'DeviceProtection_No internet service', 'DeviceProtection_Yes']}, 'TechSupport': {'type': 'onehot', 'columns': ['TechSupport_No', 'TechSupport_No internet service', 'TechSupport_Yes']}, 'StreamingTV': {'type': 'onehot', 'columns': ['StreamingTV_No', 'StreamingTV_No internet service', 'StreamingTV_Yes']}, 'StreamingMovies': {'type': 'onehot', 'columns': ['StreamingMovies_No', 'StreamingMovies_No internet service', 'StreamingMovies_Yes']}, 'Contract': {'type': 'onehot', 'columns': ['Contract_Month-to-month', 'Contract_One year', 'Contract_Two year']}, 'PaymentMethod': {'type': 'onehot', 'columns': ['PaymentMethod_Bank transfer (automatic)', 'PaymentMethod_Credit card (automatic)', 'PaymentMethod_Electronic check', 'PaymentMethod_Mailed check']}, 'TenureSegment': {'type': 'onehot', 'columns': ['TenureSegment_0-12m', 'TenureSegment_12-24m', 'TenureSegment_24-40m', 'TenureSegment_48+']}, 'TenureBin': {'type': 'label', 'encoder': LabelEncoder()}, 'CustomerValue': {'type': 'onehot', 'columns': ['CustomerValue_Low_Value', 'CustomerValue_Medium_Value', 'CustomerValue_High_Value']}, 'Tenure_Segment': {'type': 'onehot', 'columns': ['Tenure_Segment_High_Risk', 'Tenure_Segment_Medium_Risk', 'Tenure_Segment_Low_Risk', 'Tenure_Segment_Very_Low_Risk']}, 'Tenure_Category': {'type': 'label', 'encoder': LabelEncoder()}, 'Contract_Risk': {'type': 'onehot', 'columns': ['Contract_Risk_High_Risk', 'Contract_Risk_Low_Risk', 'Contract_Risk_Medium_Risk']}, 'Tenure_Risk_Score': {'type': 'onehot', 'columns': ['Tenure_Risk_Score_3', 'Tenure_Risk_Score_2', 'Tenure_Risk_Score_1', 'Tenure_Risk_Score_0']}}, 'feature_importance':                                     feature  importance\n",
      "10                MonthlyCharges_per_Tenure    0.131503\n",
      "27                      Contract_Risk_Score    0.097838\n",
      "65                  Contract_Month-to-month    0.094748\n",
      "30                      Combined_Risk_Score    0.090554\n",
      "85                  Contract_Risk_High_Risk    0.089497\n",
      "..                                      ...         ...\n",
      "68  PaymentMethod_Bank transfer (automatic)    0.000000\n",
      "74                     TenureSegment_24-40m    0.000000\n",
      "78               CustomerValue_Medium_Value    0.000000\n",
      "82                  Tenure_Segment_Low_Risk    0.000000\n",
      "89                      Tenure_Risk_Score_2    0.000000\n",
      "\n",
      "[92 rows x 2 columns], 'dataset_info': {'shape': (7021, 31), 'columns': ['MonthlyCharges_per_Tenure', 'Contract_Risk_Score', 'Contract_Month-to-month', 'Combined_Risk_Score', 'Contract_Risk_High_Risk', 'Revenue_per_Service', 'TenureBin_encoded', 'tenure', 'Charges_vs_Tenure_Avg', 'Internet_No_Support', 'Contract_Risk_Low_Risk', 'OnlineSecurity_No', 'Internet_No_Security', 'TechSupport_No', 'Tenure_Category_encoded', 'Established_Customer', 'LongContract_AutoPay', 'Contract_Two year', 'Tenure_Segment_Very_Low_Risk', 'EstimatedCLV', 'Expected_TotalCharges', 'TenureSegment_0-12m', 'Tenure_Risk_Score_0', 'MonthlyCharges', 'InternetService_Fiber optic', 'TenureSegment_48+', 'Tenure_Segment_High_Risk', 'TotalCharges', 'PaymentMethod_Electronic check', 'OnlineSecurity_No internet service', 'Churn'], 'target_column': 'Churn'}}\n",
      "Features (X): 7021\n",
      "Target (y): (7021,)\n"
     ]
    }
   ],
   "source": [
    "file = '../src/telco_churn_processed.csv'\n",
    "model_objects = '../src/telco_churn_processed_objects.pkl'\n",
    "\n",
    "load_data(file, model_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee72f86e",
   "metadata": {},
   "source": [
    "Next split will be between train data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c83162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7d92f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_data(test_size=0.2, random_state=42, stratify=True):\n",
    "    \"\"\"Splits the dataset into train and test.\"\"\"\n",
    "    global X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Check for features and targets.\n",
    "        if X is None or y is None:\n",
    "            print(f'Load the data first!')\n",
    "            return False \n",
    "\n",
    "        # Setting up stratify.\n",
    "        stratify_param = y if stratify else None\n",
    "         \n",
    "        # Train/test split.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size, random_state=random_state, stratify=stratify_param)\n",
    "        \n",
    "        print(f'TRAIN/TEST SPLIT COMPLETED!')\n",
    "        print(f'-'*50)\n",
    "        print(f'Training set: {X_train.shape}')\n",
    "        print(f'Test set: {X_test.shape}')\n",
    " \n",
    "    except Exception as e:\n",
    "        print(f'Encountered error in train/test split: {str(e)}')\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed8f52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN/TEST SPLIT COMPLETED!\n",
      "--------------------------------------------------\n",
      "Training set: (5616, 30)\n",
      "Test set: (1405, 30)\n"
     ]
    }
   ],
   "source": [
    "train_test_split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07140fa",
   "metadata": {},
   "source": [
    "After the second split, we have to handle the class imbalance that we found out on the EDA process. There are plenty of techniques to balance a dataset. \n",
    "\n",
    "We could choose between oversampling techinques (SMOTE, ADASYIN, RandomOverSampler), undersampling techniques (RamdomUnderSampler, TomekLinks) or combined over/undersampling techniques (SMOTETomek, SMOTEENN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7445e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler \n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0add1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_tech(tech='smote', sampling_strategy='auto', random_state=42):\n",
    "    \"\"\"Applies balance techniques to the dataset.\"\"\"\n",
    "    global X_train_balanced, y_train_balanced\n",
    "    \n",
    "    # First check.\n",
    "    if X_train is None or y_train is None:\n",
    "        print(f'Do the train/test split first!')\n",
    "        return False\n",
    "    \n",
    "    print(f'Applying balancing technique: {tech}')\n",
    "    print(f'-'*50)\n",
    "    \n",
    "    # Original distribution.\n",
    "    origin_dist = Counter(y_train)\n",
    "    print(f'Original distribution: {dict(origin_dist)}')\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Selecting technique.\n",
    "        if tech == 'smote':\n",
    "            sampler = SMOTE(sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "        elif tech == 'adasyn':\n",
    "            sampler = ADASYN(sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "        elif tech == 'random_over':\n",
    "            sampler = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "        elif tech == 'random_under':\n",
    "            sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "        elif tech == 'tomek':\n",
    "            sampler = TomekLinks()\n",
    "        elif tech == 'smote_tomek':\n",
    "            sampler = SMOTETomek(sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "        elif tech == 'smote_enn':\n",
    "            sampler = SMOTEENN(sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "        else:\n",
    "            print(f'Techinque {tech} not recognised.')\n",
    "            return False \n",
    "        \n",
    "        # Applying balance.\n",
    "        X_train_balanced, y_train_balanced = sampler.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Distribution after balance.\n",
    "        new_dist = Counter(y_train_balanced)\n",
    "        print(f'Balanced distribution: {dict(new_dist)}')\n",
    "        \n",
    "        # Showing changes.\n",
    "        print(f'\\n CHANGES APPLIED:')\n",
    "        print(f'-'*50)\n",
    "        print(f'Original size: {len(y_train)} samples')\n",
    "        print(f'Balance size: {len(y_train_balanced)} samples')\n",
    "        print(f'Change: {len(y_train_balanced) - len(y_train):+d} samples.')\n",
    "        \n",
    "        for class_val in [0,1]:\n",
    "            original_count = origin_dist[class_val]\n",
    "            new_count = new_dist[class_val]\n",
    "            change = new_count - original_count\n",
    "            print(f'Class {class_val}: {original_count} -> {new_count} ({change:+d})')\n",
    "        \n",
    "        # New ratio.\n",
    "        new_ratio = new_dist[0] / new_dist[1]\n",
    "        print(f'New ratio: {new_ratio:.2f}:1')\n",
    "        print(f'Dataset balanced succesfully with {tech}!')\n",
    "        return True  \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Encountered error while applying balanced technique: {str(e)}')\n",
    "        return False \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a50fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying balancing technique: smote\n",
      "--------------------------------------------------\n",
      "Original distribution: {1: 1485, 0: 4131}\n",
      "Balanced distribution: {1: 4131, 0: 4131}\n",
      "\n",
      " CHANGES APPLIED:\n",
      "--------------------------------------------------\n",
      "Original size: 5616 samples\n",
      "Balance size: 8262 samples\n",
      "Change: +2646 samples.\n",
      "Class 0: 4131 -> 4131 (+0)\n",
      "Class 1: 1485 -> 4131 (+2646)\n",
      "New ratio: 1.00:1\n",
      "Dataset balanced succesfully with smote!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_tech()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf15d9a",
   "metadata": {},
   "source": [
    "In this case i used SMOTE, but we could choose whatever technique we prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a6527",
   "metadata": {},
   "source": [
    "## Model Developing\n",
    "\n",
    "Next step will be to configure the models to be trained and make predictions.\n",
    "\n",
    "Previous to the training, i'm going to initialize the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5bb4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80d90635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models():\n",
    "    \"\"\"Initialize the models to train.\"\"\"\n",
    "    global models\n",
    "    \n",
    "    # Configurating models.\n",
    "    models = {\n",
    "        'logistic_regression' : LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
    "        'random_forest' : RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced', n_jobs=1),\n",
    "        'gradient_boosting' : GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "        'xgboost' : xgb.XGBClassifier(random_state=42, eval_metric='logloss',n_estimators=100),\n",
    "        'lightgbm': lgb.LGBMClassifier(random_state=42, n_estimators=100, verbose=-1)       \n",
    "    }\n",
    "    # Showing models.\n",
    "    print(f'Initialized models: {len(models)}')\n",
    "    print(f'-'*40)\n",
    "    for name in models.keys():\n",
    "        print(f'    * {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b414383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized models: 5\n",
      "----------------------------------------\n",
      "    * logistic_regression\n",
      "    * random_forest\n",
      "    * gradient_boosting\n",
      "    * xgboost\n",
      "    * lightgbm\n"
     ]
    }
   ],
   "source": [
    "initialize_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778a1b3c",
   "metadata": {},
   "source": [
    "Following this step, the training will start and display its metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9ab5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, f1_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "382281bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(use_balanced_data = True, cv_folds=5):\n",
    "    \"\"\"Model training with cross validation.\"\"\"\n",
    "    global results \n",
    "    # If models were not initizalize, do it.\n",
    "    if not models:\n",
    "        initialize_models()\n",
    "    \n",
    "    # Data training select. (Original or balanced)\n",
    "    if use_balanced_data and X_train_balanced is not None:\n",
    "        X_train_use = X_train_balanced\n",
    "        y_train_use = y_train_balanced\n",
    "        data_type = 'Balanced'\n",
    "    else:\n",
    "        X_train_use = X_train \n",
    "        y_train = y_train \n",
    "        data_type = 'Original'\n",
    "    \n",
    "    if X_train_use is None:\n",
    "        print(f'Do the train/test split first!')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    print(f' Model training using {data_type} data')\n",
    "    print(f'-'*50)\n",
    "    \n",
    "    cv_results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f'\\n Training {name}...')\n",
    "        try: \n",
    "            # Cross validation.\n",
    "            cv_scores = cross_val_score(model, X_train_use, y_train_use, cv=StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42), scoring='roc_auc', n_jobs=-1)\n",
    "            \n",
    "            # Training.\n",
    "            model.fit(X_train_use, y_train_use)\n",
    "            \n",
    "            # Predictions on test.\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Metrics.\n",
    "            test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            test_f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            cv_results[name] = {\n",
    "                'cv_auc_mean': cv_scores.mean(),\n",
    "                'cv_auc_std': cv_scores.std(),\n",
    "                'test_auc': test_auc,\n",
    "                'test_f1': test_f1,\n",
    "                'y_pred': y_pred,\n",
    "                'y_pred_proba' : y_pred_proba,\n",
    "                'model' : model \n",
    "            }\n",
    "            \n",
    "            print(f'    CV AUC: {cv_scores.mean():.4f} (+-{cv_scores.std():.4f}')\n",
    "            print(f'    Test AUC: {test_auc:.4f}')\n",
    "            print(f'    Test F1: {test_f1:.4f}')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Encountered error : {str(e)}')\n",
    "            cv_results[name] = None \n",
    "    \n",
    "    results['baseline'] = cv_results\n",
    "    \n",
    "    # Displaying best models sorted by CV AUC.\n",
    "    print(f'\\n MODEL RANKING (by CV AUC)')\n",
    "    print(f'-'*40)\n",
    "    valid_results = {k:v for k,v in cv_results.items() if v is not None}\n",
    "    sorted_models = sorted(valid_results.items(), key=lambda x: x[1]['cv_auc_mean'], reverse=True)\n",
    "    for i, (name, metrics) in enumerate(sorted_models, 1):\n",
    "        print(f'{i}. {name:20} - CV AUC: {metrics['cv_auc_mean']:.4f}')\n",
    "    \n",
    "    return cv_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cadaff07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model training using Balanced data\n",
      "--------------------------------------------------\n",
      "\n",
      " Training logistic_regression...\n",
      "    CV AUC: 0.8509 (+-0.0082\n",
      "    Test AUC: 0.8397\n",
      "    Test F1: 0.6109\n",
      "\n",
      " Training random_forest...\n",
      "    CV AUC: 0.9031 (+-0.0058\n",
      "    Test AUC: 0.8020\n",
      "    Test F1: 0.5534\n",
      "\n",
      " Training gradient_boosting...\n",
      "    CV AUC: 0.8839 (+-0.0077\n",
      "    Test AUC: 0.8416\n",
      "    Test F1: 0.6242\n",
      "\n",
      " Training xgboost...\n",
      "    CV AUC: 0.9032 (+-0.0066\n",
      "    Test AUC: 0.8221\n",
      "    Test F1: 0.5882\n",
      "\n",
      " Training lightgbm...\n",
      "    CV AUC: 0.9045 (+-0.0066\n",
      "    Test AUC: 0.8303\n",
      "    Test F1: 0.6174\n",
      "\n",
      " MODEL RANKING (by CV AUC)\n",
      "----------------------------------------\n",
      "1. lightgbm             - CV AUC: 0.9045\n",
      "2. xgboost              - CV AUC: 0.9032\n",
      "3. random_forest        - CV AUC: 0.9031\n",
      "4. gradient_boosting    - CV AUC: 0.8839\n",
      "5. logistic_regression  - CV AUC: 0.8509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logistic_regression': {'cv_auc_mean': np.float64(0.8508673223216896),\n",
       "  'cv_auc_std': np.float64(0.008243742609650486),\n",
       "  'test_auc': np.float64(0.8396881408154555),\n",
       "  'test_f1': 0.6109324758842444,\n",
       "  'y_pred': array([1, 1, 1, ..., 0, 1, 0], shape=(1405,)),\n",
       "  'y_pred_proba': array([0.8214583 , 0.74106214, 0.7113917 , ..., 0.05071972, 0.79647604,\n",
       "         0.4012333 ], shape=(1405,)),\n",
       "  'model': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)},\n",
       " 'random_forest': {'cv_auc_mean': np.float64(0.9031361426651898),\n",
       "  'cv_auc_std': np.float64(0.0057638094120781155),\n",
       "  'test_auc': np.float64(0.8020016862879805),\n",
       "  'test_f1': 0.5534105534105535,\n",
       "  'y_pred': array([0, 0, 1, ..., 0, 1, 0], shape=(1405,)),\n",
       "  'y_pred_proba': array([0.4       , 0.39      , 0.93883333, ..., 0.        , 0.83      ,\n",
       "         0.1       ], shape=(1405,)),\n",
       "  'model': RandomForestClassifier(class_weight='balanced', n_jobs=1, random_state=42)},\n",
       " 'gradient_boosting': {'cv_auc_mean': np.float64(0.8839301584242223),\n",
       "  'cv_auc_std': np.float64(0.007681054150387154),\n",
       "  'test_auc': np.float64(0.8416268515337934),\n",
       "  'test_f1': 0.6241758241758242,\n",
       "  'y_pred': array([1, 1, 1, ..., 0, 1, 0], shape=(1405,)),\n",
       "  'y_pred_proba': array([0.78557008, 0.69291903, 0.76516346, ..., 0.05960615, 0.75829205,\n",
       "         0.22281014], shape=(1405,)),\n",
       "  'model': GradientBoostingClassifier(random_state=42)},\n",
       " 'xgboost': {'cv_auc_mean': np.float64(0.9031852263697544),\n",
       "  'cv_auc_std': np.float64(0.006628575612175973),\n",
       "  'test_auc': np.float64(0.8220692939449771),\n",
       "  'test_f1': 0.5882352941176471,\n",
       "  'y_pred': array([1, 0, 1, ..., 0, 1, 0], shape=(1405,)),\n",
       "  'y_pred_proba': array([0.77775913, 0.4339265 , 0.7659058 , ..., 0.00319104, 0.8323631 ,\n",
       "         0.21137483], shape=(1405,), dtype=float32),\n",
       "  'model': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, feature_weights=None, gamma=None,\n",
       "                grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "                num_parallel_tree=None, ...)},\n",
       " 'lightgbm': {'cv_auc_mean': np.float64(0.9045211197689605),\n",
       "  'cv_auc_std': np.float64(0.006617215855078947),\n",
       "  'test_auc': np.float64(0.8303224765533106),\n",
       "  'test_f1': 0.6173708920187794,\n",
       "  'y_pred': array([1, 1, 1, ..., 0, 1, 0], shape=(1405,)),\n",
       "  'y_pred_proba': array([0.74896524, 0.55356424, 0.8234361 , ..., 0.00923871, 0.82316827,\n",
       "         0.25591111], shape=(1405,)),\n",
       "  'model': LGBMClassifier(random_state=42, verbose=-1)}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
